// These constants represent the RISC-V ELF and the image ID generated by risc0-build.
// The ELF is used for proving and the ID is used for verification.
use anyhow::{anyhow, Context, Error};
use avail_subxt::config::Header;
use nexus_core::{
    agg_types::{AggregatedTransaction, InitTransaction, SubmitProofTransaction},
    db::NodeDB,
    mempool::Mempool,
    state::{merkle_store, MerkleStore, VmState},
    state_machine::StateMachine,
    types::{
        AvailHeader, HeaderStore, NexusHeader, Proof as NexusProof, RollupPublicInputsV2,
        TransactionV2, TransactionZKVM, TxParamsV2, H256,
    },
    zkvm::{
        traits::{ZKVMEnv, ZKVMProof, ZKVMProver},
        ProverMode,
    },
};
use host::AvailToNexusPointer;
use host::execute_batch;

#[cfg(any(feature = "risc0"))]
use nexus_core::zkvm::risczero::{RiscZeroProof as Proof, RiscZeroProver as Prover, ZKVM};

#[cfg(any(feature = "sp1"))]
use nexus_core::zkvm::sp1::{Sp1Proof as Proof, Sp1Prover as Prover, SP1ZKVM as ZKVM};

#[cfg(any(feature = "risc0"))]
use prover::{NEXUS_RUNTIME_ELF, NEXUS_RUNTIME_ID};
use relayer::Relayer;
use rocksdb::{Options, DB};
use serde::ser::StdError;
use serde::{Deserialize, Serialize};
use sp_runtime::DeserializeOwned;
use std::net::SocketAddr;
use std::str::FromStr;
use std::{collections::HashMap, sync::Arc};
use std::{env::args, fmt::Debug as DebugTrait};
use tokio::sync::Mutex;
use tokio::time::{sleep, Duration};
use warp::Filter;

use crate::rpc::routes;

mod rpc;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = args().collect();
    let dev_flag = args.iter().any(|arg| arg == "--dev");
    if dev_flag {
        println!("⚠️ Running in dev mode. Proofs are not valid");
    }

    let prover_mode = if dev_flag {
        ProverMode::MockProof
    } else {
        ProverMode::Compressed
    };

    let node_db: NodeDB = NodeDB::from_path(&String::from("./db/node_db"));
    let mut db_options = Options::default();
    db_options.create_if_missing(true);

    let old_state_root: H256 = match node_db.get_current_root()? {
        Some(i) => i,
        None => H256::zero(),
    };

    let state = Arc::new(Mutex::new(VmState::new(&String::from("./db/runtime_db"))));

    let db = Arc::new(Mutex::new(node_db));
    let db_clone = db.clone();
    let db_clone_2 = db.clone();
    let mut state_machine = StateMachine::<ZKVM, Proof>::new(state.clone());

    let relayer_mutex = Arc::new(Mutex::new(Relayer::new()));

    let rt = tokio::runtime::Runtime::new().unwrap();

    let rt = tokio::runtime::Runtime::new().unwrap();
    rt.block_on(async move {
        let receiver = {
            let mut relayer = relayer_mutex.lock().await;

            relayer.receiver()
        };
        let mempool = Mempool::new();
        let mempool_clone = mempool.clone();
        let relayer_handle = tokio::spawn(async move {
            let cloned_relayer = relayer_mutex.lock().await;
            let start_height: u32 = {
                let db_lock = db_clone_2.lock().await;

                let avail_hash: Option<H256> = match db_lock.get::<HeaderStore>(b"previous_headers")
                {
                    //Can do unwrap below as an empty store would not be stored.
                    Ok(Some(i)) => Some(i.first().unwrap().avail_header_hash),
                    Ok(None) => None,
                    Err(_) => panic!("Could not access db"),
                };

                if let Some(hash) = avail_hash {
                    let height = match db_lock.get::<AvailToNexusPointer>(hash.as_slice()) {
                        Ok(Some(i)) => i.number,
                        Ok(None) => panic!("Node DB error. Cannot find mapping to avail -> nexus block for already processed block"),
                        Err(e) => {
                            println!("{:?}", e);

                            panic!("Node DB error. Cannot find mapping to avail -> nexus block")
                        },
                    } + 1;

                    height
                } else {
                    10000
                }
            };

            cloned_relayer.start(start_height).await;
        });

        let execution_engine = tokio::spawn(async move {
            while let Some(header) = receiver.lock().await.recv().await {
                let mut old_headers: HeaderStore = {
                    let db_lock = db.lock().await;
                    match db_lock.get(b"previous_headers") {
                        Ok(Some(i)) => i,
                        Ok(None) => HeaderStore::new(32),
                        Err(_) => break,
                    }
                };
                let (txs, index) = mempool_clone.get_current_txs().await;

                println!(
                    "Number of txs for height {} -- {}",
                    header.number,
                    txs.len()
                );
                
                // capture txs , state_machine , availHeaeer an heaerstore in another file in json format
                match execute_batch::<Prover, Proof, ZKVM>(
                    &txs,
                    &mut state_machine,
                    &AvailHeader::from(&header),
                    &mut old_headers,
                    prover_mode.clone()
                ).await {                    
                    Ok((mut proof, result)) => { //assumption that the proof will be given as succinct here.
                                               
                        let compressed_proof = proof.compress();

                        let db_lock = db.lock().await;
                        let nexus_hash: H256 = result.hash();

                        db_lock.put(b"previous_headers", &old_headers).unwrap();
                        db_lock.put(
                            result.avail_header_hash.as_slice(),
                            &AvailToNexusPointer {
                                number: header.number,
                                nexus_hash: nexus_hash.clone(),
                            },
                        ).unwrap();
                        db_lock.put(nexus_hash.as_slice(), &result).unwrap();

                        db_lock.set_current_root(&result.state_root).unwrap();
                        if let Some(i) = index {
                            mempool_clone.clear_upto_tx(i).await;
                        }

                        println!(
                            "✅ Processed batch: {:?}, avail height: {:?}",
                            result, header.number
                        );
                    }
                    Err(e) => {
                        println!("Breaking because of error {:?}", e);
                        break;
                    }
                };

                tokio::time::sleep(Duration::from_secs(5)).await;
            }
        });

        //Server part//
        let routes = routes(mempool, db_clone, state.clone());
        let cors = warp::cors()
            .allow_any_origin()
            .allow_methods(vec!["POST"])
            .allow_headers(vec!["content-type"]);
        let routes = routes.with(cors);
        let server: tokio::task::JoinHandle<()> = tokio::spawn(async move {
            let address =
                SocketAddr::from_str(format!("{}:{}", String::from("127.0.0.1"), 7000).as_str())
                    .context("Unable to parse host address from config")
                    .unwrap();

            println!("RPC Server running on: {:?}", &address);
            warp::serve(routes).run(address).await;
        });

        let result = tokio::try_join!(server, execution_engine, relayer_handle);

        match result {
            Ok((_, _, _)) => {
                println!("Exiting node, should not have happened.");
            }
            Err(e) => {
                println!("Exiting node, should not have happened. {:?}", e);
            }
        }
    });

    Ok(())
}
 
// #[derive(Clone, Debug, Serialize, Deserialize)]
// pub struct AvailToNexusPointer {
//     number: u32,
//     nexus_hash: H256,
// }

// #[derive(Clone, Debug)]
// pub struct BatchesToAggregate(Arc<Mutex<Vec<(Vec<InitTransaction>, AggregatedTransaction)>>>);

// impl BatchesToAggregate {
//     pub fn new() -> Self {
//         Self(Arc::new(Mutex::new(vec![])))
//     }

//     pub async fn add_batch(&self, batch: (Vec<InitTransaction>, AggregatedTransaction)) {
//         self.0.lock().await.push(batch);
//     }

//     pub async fn get_next_batch(&self) -> Option<(Vec<InitTransaction>, AggregatedTransaction)> {
//         Some(self.0.lock().await.first()?.clone())
//     }

//     pub async fn remove_first_batch(&self) {
//         let mut list = &mut self.0.lock().await;

//         if !list.is_empty() {
//             &list.remove(0);
//         } else {
//             // Handle case where index exceeds the length of tx_list
//             list.clear();
//         }
//     }
// }
