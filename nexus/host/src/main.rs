// These constants represent the RISC-V ELF and the image ID generated by risc0-build.
// The ELF is used for proving and the ID is used for verification.
use anyhow::{anyhow, Context, Error};
use avail_subxt::config::Header;
use nexus_core::{
    agg_types::{AggregatedTransaction, InitTransaction, SubmitProofTransaction},
    db::NodeDB,
    mempool::Mempool,
    state::{merkle_store, MerkleStore, VmState},
    state_machine::StateMachine,
    types::{
        AvailHeader, HeaderStore, NexusHeader, Proof as NexusProof, RollupPublicInputsV2,
        TransactionV2, TransactionZKVM, TxParamsV2, H256,
    },
    zkvm::{
        risczero::{Proof, RiscZeroProver, ZKVM},
        traits::{ZKProof, ZKVMEnv, ZKVMProver},
    },
};
use prover::{NEXUS_RUNTIME_ELF, NEXUS_RUNTIME_ID};
use relayer::Relayer;
use risc0_zkvm::{
    default_executor, default_prover, serde::from_slice, ExecutorEnv, Journal, Receipt,
};
use rocksdb::{Options, DB};
use serde::{Deserialize, Serialize};
use sp_runtime::DeserializeOwned;
use std::{
    collections::HashMap,
    sync::{Arc, Mutex as StdMutex},
};
use tokio::time::{sleep, Duration};
use tokio::{self, sync::Mutex};
use warp::Filter;

use serde::ser::StdError;
use std::fmt::Debug as DebugTrait;
use std::net::SocketAddr;
use std::str::FromStr;

use crate::rpc::routes;

mod rpc;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let node_db: NodeDB = NodeDB::from_path(&String::from("./db/node_db"));
    let mut db_options = Options::default();
    db_options.create_if_missing(true);

    let old_state_root: H256 = match node_db.get_current_root()? {
        Some(i) => i,
        None => H256::zero(),
    };

    let state = Arc::new(Mutex::new(VmState::new(&String::from("./db/runtime_db"))));
    let state_clone = state.clone();

    let db = Arc::new(Mutex::new(node_db));
    let db_clone = db.clone();
    let db_clone_2 = db.clone();
    let relayer_mutex = Arc::new(Mutex::new(Relayer::new()));

    let rt = tokio::runtime::Runtime::new().unwrap();
    rt.block_on(async move {
        let receiver = {
            let mut relayer = relayer_mutex.lock().await;

            relayer.receiver()
        };
        let mempool = Mempool::new();
        let mempool_clone = mempool.clone();
        let relayer_handle = tokio::spawn(async move {
            let cloned_relayer = relayer_mutex.lock().await;
            let start_height: u32 = {
                let db_lock = db_clone_2.lock().await;

                let avail_hash: Option<H256> = match db_lock.get::<HeaderStore>(b"previous_headers")
                {
                    //Can do unwrap below as an empty store would not be stored.
                    Ok(Some(i)) => Some(i.first().unwrap().avail_header_hash),
                    Ok(None) => None,
                    Err(_) => panic!("Could not access db"),
                };

                if let Some(hash) = avail_hash {
                    let height = match db_lock.get::<AvailToNexusPointer>(hash.as_slice()) {
                        Ok(Some(i)) => i.number,
                        Ok(None) => panic!("Node DB error. Cannot find mapping to avail -> nexus block for already processed block"),
                        Err(e) => {
                            println!("{:?}", e);

                            panic!("Node DB error. Cannot find mapping to avail -> nexus block")
                        },
                    } + 1;

                    height
                } else {
                    10000
                }
            };

            cloned_relayer.start(start_height).await;
        });

        let execution_engine = tokio::spawn(async move {
            while let Some(header) = receiver.lock().await.recv().await {
                let mut old_headers: HeaderStore = {
                    let db_lock = db.lock().await;
                    match db_lock.get(b"previous_headers") {
                        Ok(Some(i)) => i,
                        Ok(None) => HeaderStore::new(32),
                        Err(_) => break,
                    }
                };
                let (txs, index) = mempool_clone.get_current_txs().await;

                println!(
                    "Number of txs for height {} -- {}",
                    header.number,
                    txs.len()
                );

                match execute_batch::<RiscZeroProver, Proof, ZKVM>(
                    &txs,
                    state.clone(),
                    &AvailHeader::from(&header),
                    &mut old_headers,
                ).await {
                    Ok((_, result)) => {
                        let db_lock = db.lock().await;
                        let nexus_hash: H256 = result.hash();

                        // db_lock.put(b"previous_headers", &old_headers).unwrap();
                        // db_lock.put(
                        //     result.avail_header_hash.as_slice(),
                        //     &AvailToNexusPointer {
                        //         number: header.number,
                        //         nexus_hash: nexus_hash.clone(),
                        //     },
                        // ).unwrap();
                        // db_lock.put(nexus_hash.as_slice(), &result).unwrap();

                        // db_lock.set_current_root(&result.state_root).unwrap();
                        // if let Some(i) = index {
                        //     mempool_clone.clear_upto_tx(i).await;
                        // }

                        // println!(
                        //     "âœ… Processed batch: {:?}, avail height: {:?}",
                        //     result, header.number
                        // );
                    }
                    Err(e) => {
                        println!("Breaking because of error {:?}", e);
                        break;
                    }
                };

                tokio::time::sleep(Duration::from_secs(5)).await;
            }
        });

        //Server part//
        let routes = routes(mempool, db_clone, state_clone);
        let cors = warp::cors()
            .allow_any_origin()
            .allow_methods(vec!["POST"])
            .allow_headers(vec!["content-type"]);
        let routes = routes.with(cors);
        let server: tokio::task::JoinHandle<()> = tokio::spawn(async move {
            let address =
                SocketAddr::from_str(format!("{}:{}", String::from("127.0.0.1"), 7000).as_str())
                    .context("Unable to parse host address from config")
                    .unwrap();

            println!("RPC Server running on: {:?}", &address);
            warp::serve(routes).run(address).await;
        });

        let result = tokio::try_join!(server, execution_engine, relayer_handle);

        match result {
            Ok((_, _, _)) => {
                println!("Exiting node, should not have happened.");
            }
            Err(e) => {
                println!("Exiting node, should not have happened. {:?}", e);
            }
        }
    });

    Ok(())
}

async fn execute_batch<
    Z: ZKVMProver<P>,
    P: ZKProof + Serialize + Clone + DebugTrait,
    E: ZKVMEnv,
>(
    txs: &Vec<TransactionV2>,
    state: Arc<Mutex<VmState>>,
    header: &AvailHeader,
    header_store: &mut HeaderStore,
) -> Result<(P, NexusHeader), Error> {
    let mut state_machine = StateMachine::<ZKVM, Proof>::new(state.clone());

    let state_update = state_machine
        .execute_batch(&header, header_store, &txs, 0)
        .await?;

    let mut zkvm_prover = Z::new(NEXUS_RUNTIME_ELF.clone().to_vec());

    let zkvm_txs: Result<Vec<TransactionZKVM>, anyhow::Error> = txs
        .iter()
        .map(|tx| {
            if let TxParamsV2::SubmitProof(submit_proof_tx) = &tx.params {
                //TODO: Remove transactions that error out from mempool
                let proof = submit_proof_tx.proof.clone();

                let receipt: P = P::try_from(proof).unwrap();
                let pre_state = match state_update.1.pre_state.get(&submit_proof_tx.app_id.0) {
                    Some(i) => i,
                    None => {
                        return Err(anyhow!(
                         "Incorrect StateUpdate computed. Cannot find state for AppAccountId: {:?}",
                         submit_proof_tx.app_id
                     ))
                    }
                };

                zkvm_prover.add_proof_for_recursion(receipt).unwrap();
            }

            Ok(TransactionZKVM {
                signature: tx.signature.clone(),
                params: tx.params.clone(),
            })
        })
        .collect();

    let zkvm_txs = zkvm_txs?;

    zkvm_prover.add_input(&zkvm_txs).unwrap();
    zkvm_prover.add_input(&state_update.1).unwrap();
    zkvm_prover.add_input(&header).unwrap();
    zkvm_prover.add_input(&header_store).unwrap();

    let proof = zkvm_prover.prove()?;
    //let result: NexusHeader = from_slice(&receipt.journal.bytes).unwrap();
    let result: NexusHeader = proof.public_inputs()?;

    header_store.push_front(&result);

    match state_update.0 {
        Some(i) => {
            state_machine
                .commit_state(&result.state_root, &i.node_batch, 0)
                .await?;
        }
        None => (),
    }

    Ok((proof, result))
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct AvailToNexusPointer {
    number: u32,
    nexus_hash: H256,
}

// #[derive(Clone, Debug)]
// pub struct BatchesToAggregate(Arc<Mutex<Vec<(Vec<InitTransaction>, AggregatedTransaction)>>>);

// impl BatchesToAggregate {
//     pub fn new() -> Self {
//         Self(Arc::new(Mutex::new(vec![])))
//     }

//     pub async fn add_batch(&self, batch: (Vec<InitTransaction>, AggregatedTransaction)) {
//         self.0.lock().await.push(batch);
//     }

//     pub async fn get_next_batch(&self) -> Option<(Vec<InitTransaction>, AggregatedTransaction)> {
//         Some(self.0.lock().await.first()?.clone())
//     }

//     pub async fn remove_first_batch(&self) {
//         let mut list = &mut self.0.lock().await;

//         if !list.is_empty() {
//             &list.remove(0);
//         } else {
//             // Handle case where index exceeds the length of tx_list
//             list.clear();
//         }
//     }
// }
