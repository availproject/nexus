// These constants represent the RISC-V ELF and the image ID generated by risc0-build.
// The ELF is used for proving and the ID is used for verification.
use anyhow::{Context, Error};
use nexus_core::{
    agg_types::{AggregatedTransaction, InitTransaction, SubmitProofTransaction},
    db::NodeDB,
    mempool::Mempool,
    state_machine::StateMachine,
    types::{
        AvailHeader, HeaderStore, NexusHeader, TransactionV2, TransactionZKVM, TxParamsV2, H256,
    },
};
use prover::{NEXUS_RUNTIME_ELF, NEXUS_RUNTIME_ID};
use relayer::Relayer;
use risc0_zkvm::{
    default_executor, default_prover, serde::from_slice, ExecutorEnv, Journal, Receipt,
};
use std::sync::Arc;
use tokio::time::{sleep, Duration};
use tokio::{self, sync::Mutex};
use warp::Filter;

use std::net::SocketAddr;
use std::str::FromStr;

use crate::rpc::routes;

pub mod rpc;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let node_db: NodeDB = NodeDB::from_path(String::from("./node_db"));
    let old_state_root: H256 = match node_db.get_current_root()? {
        Some(i) => i,
        None => H256::zero(),
    };
    let db = Arc::new(Mutex::new(node_db));
    let db_clone = db.clone();
    let db_clone_2 = db.clone();
    let mut state_machine = StateMachine::new(old_state_root, "./runtime_db");
    let relayer_mutex = Arc::new(Mutex::new(Relayer::new()));

    let rt = tokio::runtime::Runtime::new().unwrap();
    rt.block_on(async move {
        let receiver = {
            let mut relayer = relayer_mutex.lock().await;

            relayer.receiver()
        };
        let mempool = Mempool::new();
        let mempool_clone = mempool.clone();
        let relayer_handle = tokio::spawn(async move {
            let cloned_relayer = relayer_mutex.lock().await;
            let start_height = match db_clone_2
                .lock()
                .await
                .get::<HeaderStore>(b"previous_headers")
            {
                Ok(Some(i)) => i.first().number + 1,
                Ok(None) => 606460,
                Err(_) => panic!("Could access db"),
            };

            cloned_relayer.start(start_height).await;
        });

        let execution_engine = tokio::spawn(async move {
            while let Some(header) = receiver.lock().await.recv().await {
                let mut old_headers: HeaderStore = {
                    let db_lock = db.lock().await;
                    match db_lock.get(b"previous_headers") {
                        Ok(Some(i)) => i,
                        Ok(None) => HeaderStore::new(32),
                        Err(_) => break,
                    }
                };
                let (txs, index) = mempool_clone.get_current_txs().await;

                println!(
                    "Number of txs for height {} -- {}",
                    header.number,
                    txs.len()
                );

                match execute_batch(
                    &txs,
                    &mut state_machine,
                    &AvailHeader::from(&header),
                    &mut old_headers,
                ) {
                    Ok((_, headers, result)) => {
                        let db_lock = db.lock().await;

                        db_lock.put(b"previous_headers", &headers).unwrap();

                        db_lock.set_current_root(&result.state_root).unwrap();
                        if let Some(i) = index {
                            mempool_clone.clear_upto_tx(i).await;
                        }

                        println!(
                            "âœ… Processed batch, the state root now is {:?}, Avail height: {}, Avail hash: {:?}",
                            result.state_root,
                            headers.first().number, 
                            headers.first().hash(),
                        );
                    }
                    Err(e) => {
                        println!("Breaking because of error {:?}", e);
                        break;
                    }
                };
            }
        });

        //Server part//
        let routes = routes(mempool, db_clone);
        let cors = warp::cors()
            .allow_any_origin()
            .allow_methods(vec!["POST"])
            .allow_headers(vec!["content-type"]);
        let routes = routes.with(cors);
        let server: tokio::task::JoinHandle<()> = tokio::spawn(async move {
            let address =
                SocketAddr::from_str(format!("{}:{}", String::from("127.0.0.1"), 7000).as_str())
                    .context("Unable to parse host address from config")
                    .unwrap();

            println!("RPC Server running on: {:?}", &address);
            warp::serve(routes).run(address).await;
        });

        let result = tokio::try_join!(server, execution_engine, relayer_handle);

        match result {
            Ok((_, _, _)) => {
                println!("Exiting node, should not have happened.");
            }
            Err(e) => {
                println!("Exiting node, should not have happened. {:?}", e);
            }
        }
    });

    Ok(())
}

fn execute_batch(
    txs: &Vec<TransactionV2>,
    state_machine: &mut StateMachine,
    header: &AvailHeader,
    header_store: &mut HeaderStore,
) -> Result<(Receipt, HeaderStore, NexusHeader), Error> {
    let mut cloned_old_headers = header_store.clone();
    let state_update = state_machine.execute_batch(&header, &mut cloned_old_headers, &txs)?;

    let mut env_builder = ExecutorEnv::builder();

    let zkvm_txs: Vec<TransactionZKVM> = txs
        .iter()
        .map(|tx| {
            if let TxParamsV2::SubmitProof(submit_proof_tx) = &tx.params {
                let proof = match &tx.proof {
                    Some(i) => i,
                    None => unreachable!("Proof cannot be empty if submit proof tx."),
                };
                let receipt = Receipt {
                    inner: proof.clone(),
                    journal: Journal {
                        //TODO: remove unwrap below
                        bytes: bincode::serialize(&submit_proof_tx.public_inputs).unwrap(),
                    },
                };

                env_builder.add_assumption(receipt);
            }

            TransactionZKVM {
                signature: tx.signature.clone(),
                params: tx.params.clone(),
            }
        })
        .collect();
    //Proof generation part.
    let env = env_builder
        .write(&zkvm_txs)
        .unwrap()
        .write(&state_update)
        .unwrap()
        .write(&header)
        .unwrap()
        .write(&header_store)
        .unwrap()
        .build()
        .unwrap();
    let prover = default_prover();
    let receipt = prover.prove(env, NEXUS_RUNTIME_ELF)?;
    let result: NexusHeader = from_slice(&receipt.journal.bytes).unwrap();

    Ok((receipt, cloned_old_headers, result))
}

// #[derive(Clone, Debug)]
// pub struct BatchesToAggregate(Arc<Mutex<Vec<(Vec<InitTransaction>, AggregatedTransaction)>>>);

// impl BatchesToAggregate {
//     pub fn new() -> Self {
//         Self(Arc::new(Mutex::new(vec![])))
//     }

//     pub async fn add_batch(&self, batch: (Vec<InitTransaction>, AggregatedTransaction)) {
//         self.0.lock().await.push(batch);
//     }

//     pub async fn get_next_batch(&self) -> Option<(Vec<InitTransaction>, AggregatedTransaction)> {
//         Some(self.0.lock().await.first()?.clone())
//     }

//     pub async fn remove_first_batch(&self) {
//         let mut list = &mut self.0.lock().await;

//         if !list.is_empty() {
//             &list.remove(0);
//         } else {
//             // Handle case where index exceeds the length of tx_list
//             list.clear();
//         }
//     }
// }
